---
title: "Can't survive without knowing this guy"
collection: portfolio
date: "2024-10-05"
permalink: /portfolio/dlstudy
venue: ''
type: Self Study
location: Anywhere
---

This article focuses mainly on unsupervised deep learning (DL) methods, and follows the awesome guide book [Understanding Deep Learning] by Simon J.D Prince, but is written in my own words. The problem to solve is to find a latent variable $\boldsymbol{z}$ for the observed $\boldsymbol{x}$ (more detailed description of the problem can be found [here], link will be added). This could be also solved by change-point detection method (CPD) as well, so we will open to CPD too. 

# What's Generative Models?
Unsupervised learning usually finds the underlying structure of the observed data, and this underlying structure can be expressed as the latent variable $\boldsymbol{z}$. We know the $\boldsymbol{z}$ means we have the information about its distribution $Pr(\boldsymbol{z})$. Using the distribution, we can generate the samples of $\boldsymbol{z}$, then using some relationship (we should learn this relationship while we were learning about $\boldsymbol{z}$) between $\boldsymbol{z}$ and $\boldsymbol{x}$ (observed data), we have another data that is generated by our model. The examples of the generative models are: *generative adversarial networks*, *normalizing flows*, *variational autoencoders*, and *diffusion models*. Let us look at the models one by one.

# Autoencoders
This one might be the most appropriate for our data which has a complex temporal dependence. 

## Latent Variable Models
When we do not have a direct access to the $p(x)$, we use a latent variable $z$ on one condition that we can relatively easily model the $p(x,z)$.




